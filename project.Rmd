---
title: "Classifying Exercise Technique Using Biometric Sensor Data"
author: "Jeff Spoelstra"
date: "`r Sys.Date()`"
output: 
  html_document: 
    fig_height: 5
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(include=FALSE, echo=FALSE, warning=FALSE, message=FALSE,
                      fig.width=6, fig.height=5, fig.pos="H")
options(digits=5, scipen=6)
```
```{r load-packages}
library(knitr)
library(data.table)
library(dplyr)
library(caret)
library(randomForest)
```
```{r load-training-data}
# NOTE: source data file must be located in the working directory with the .Rmd file.
rawtrain <- fread('./pml-training.csv', na.strings=c('NA', '', '#DIV/0!'))
```
```{r preproc-training-data}
# Set the random seed in order that all executions generate the same results.
set.seed(3514)

# Save the random number seed in the cache so that the cache is invalidated if
# someone knits this file using a different seed than the one stored in the cache.
knitr::opts_chunk$set(cache.extra=rand_seed)

# Slice off the non-numeric data we don't care about.
ptrndata <- subset(rawtrain, select=c(8:160))

# Convert all numbers stored as character strings into real numeric values.
ptrndata[,1:152] <- as.data.frame(sapply(subset(ptrndata, select=c(1:152)), as.numeric))

# Set all NA values to zero. Note that we could have imputed missing data in some
# way, but using zero produced excellent results in the model's prediction accuracy.
ptrndata[is.na(ptrndata)] <- 0

# Partition the training data set provided into a true training set and a 
# model validation set.
inTrain <- createDataPartition(y=ptrndata$classe, p=0.7, list=FALSE)
tdata <- ptrndata[inTrain,]
vdata <- ptrndata[-inTrain,]
```

##Introduction

The intent of this analysis is to use machine learning techniques to identify specific physical exercise activities from data aquired by wearable biometric sensors. In particular, to be able to distinguish correct from incorrect methods of performing a bicep curl using a dumbbell weight.

The data for this analysis comes from the Human Activity Recognition project. Details about the project and the data available can be found on the [project web site](http://groupware.les.inf.puc-rio.br/har).

Multiple models were tested to see which might yield the highest accuracy on a blind test data set. The final model selected has a suprisingly high accuracy, and in fact achieved a perfect score categorizing correct/incorrect exercises using the test data.

##Exploratory Data Analysis and Preparation

This analysis was based on the Weight Lifting Exercise (WLE) data set available at the web site noted previously. The goal of the WLE data set is to provide data with which to train and test models for predicting correct and incorrect techniques for doing dumbbell bicep curl lifts. The raw data is collected from biometric devices worn by a test subject while doing the exercises. The devices provide real-time accelerometer, gyroscope, and magnetic field measurements.

The data consists of a time sequence of raw measurement data interjected with various summary statistics (maximums, minimums, averages, etc.) at regular intervals ("windows") in time. For this analysis, the summarized data were not used because they comprised too small a percentage of the overall data set (`r format((406/19622)*100, big.mark=",", digits=3, scientific=FALSE)`%). Ignoring the summary data, only a small amount of the remaining measurements are NA (approx. `r format(((sum(is.na(rawtrain))-((25*4)*(19622-406)))/(19622*160))*100, big.mark=",", digits=3, scientific=FALSE)`%). These values were set to zero for the model fitting. This proved very effective, so no other means of imputing missing values were tried.

Each observation in the data contains an indication of the bicep curl technique being demonstrated/measured at that point in the time sequence. Five curl exercise techniques were singled out in the research: one "correct" technique and four common "incorrect" techniques. The particular technique being used is the response variable to be classified/predicted by the model. 

##Model Training
```{r train-model1, cache=TRUE}
modRF1 <- train(classe ~ 
                roll_belt+pitch_belt+yaw_belt+total_accel_belt+
                roll_arm+pitch_arm+yaw_arm+total_accel_arm+
                roll_dumbbell+pitch_dumbbell+yaw_dumbbell+total_accel_dumbbell+
                roll_forearm+pitch_forearm+yaw_forearm+total_accel_forearm,
                method='rf', 
                data=tdata, ntree=100, importance=TRUE, 
                trControl=trainControl(method='cv', number=10))
```
```{r train-model2, cache=TRUE}
modRF2 <- train(classe ~ 
                gyros_belt_x+gyros_belt_y+gyros_belt_z+
                accel_belt_x+accel_belt_y+accel_belt_z+
                magnet_belt_x+magnet_belt_y+magnet_belt_z+
                gyros_arm_x+gyros_arm_y+gyros_arm_z+
                accel_arm_x+accel_arm_y+accel_arm_z+
                magnet_arm_x+magnet_arm_y+magnet_arm_z+
                gyros_dumbbell_x+gyros_dumbbell_y+gyros_dumbbell_z+
                accel_dumbbell_x+accel_dumbbell_y+accel_dumbbell_z+
                magnet_dumbbell_x+magnet_dumbbell_y+magnet_dumbbell_z+
                gyros_forearm_x+gyros_forearm_y+gyros_forearm_z+
                accel_forearm_x+accel_forearm_y+accel_forearm_z+
                magnet_forearm_x+magnet_forearm_y+magnet_forearm_z,
                method='rf', 
                data=tdata, ntree=100, importance=TRUE, 
                trControl=trainControl(method='cv', number=10))
```
```{r train-model3, cache=TRUE}
modRF3 <- train(classe ~ 
                roll_belt+pitch_belt+yaw_belt+total_accel_belt+
                gyros_belt_x+gyros_belt_y+gyros_belt_z+
                accel_belt_x+accel_belt_y+accel_belt_z+
                magnet_belt_x+magnet_belt_y+magnet_belt_z+
                roll_arm+pitch_arm+yaw_arm+total_accel_arm+
                gyros_arm_x+gyros_arm_y+gyros_arm_z+
                accel_arm_x+accel_arm_y+accel_arm_z+
                magnet_arm_x+magnet_arm_y+magnet_arm_z+
                roll_dumbbell+pitch_dumbbell+yaw_dumbbell+total_accel_dumbbell+
                gyros_dumbbell_x+gyros_dumbbell_y+gyros_dumbbell_z+
                accel_dumbbell_x+accel_dumbbell_y+accel_dumbbell_z+
                magnet_dumbbell_x+magnet_dumbbell_y+magnet_dumbbell_z+
                roll_forearm+pitch_forearm+yaw_forearm+total_accel_forearm+
                gyros_forearm_x+gyros_forearm_y+gyros_forearm_z+
                accel_forearm_x+accel_forearm_y+accel_forearm_z+
                magnet_forearm_x+magnet_forearm_y+magnet_forearm_z,
                method='rf', 
                data=tdata, ntree=100, importance=TRUE, 
                trControl=trainControl(method='cv', number=10))
```

The WLE training data set provided was randomly divided into a working training set (hereafter referred to as the training set) and a validation set. A 70/30 split was used.

A random forest method with maximum of 100 trees and 10-fold cross-validation was used to fit three separate models to the training set. The random forest method was chosen because it is known to be highly effective at creating very accurate models, and because it is the method used by the authors of the original paper describing the WLE research project (refer to the WLE web site noted previously). Originally, 500 trees were used for model fitting, but further review indicated that 100 trees was a sufficient number - and had the additional benefit of reducing processing time for model fitting (see chart below).

```{r chart-trees, include=TRUE}
plot(modRF3$finalModel, main='Model #3\nError Rate vs. Number of Trees Used')
```

Three models were defined based upon a visual review of the raw data. Two types of data were apparent: 1) raw measurements from the accelerometers, gyroscopes, and magnetic field sensors in the biometric devices; and 2) higher-level data for each device in the form of roll, pitch, and yaw positions along with a measure of total acceleration. Model #1 fit only the roll, pitch, yaw, and total acceleration measurements as predictors for curl technique. Model #2 fit only the accelerometer, gyroscope, and magnetic measurements as predictors. Model #3 fit all of the measurements together.

Each of the models performed extremely well in terms of classification accuracy on the training set. All were above 0.97 in accuracy. The best model proved to be model #3 with an accuracy of `r format(modRF3$results$Accuracy[2], big.mark=",", digits=3, scientific=FALSE)`. Second best was model #1 using just the roll, pitch, yaw, and total acceleration measurements. Such high accuracy rates led to concerns of possibly overfitting the test data, but that proved to not be the case during the model validation step.

The information below shows the model training results and a chart of the top 10 classifying variables with their importance to selecting each exercise technique (labeled A thru E).

```{r display-model, include=TRUE}
print(modRF3$finalModel)
#plot(modRF3)
#plot(modRF3$finalModel)
plot(varImp(modRF3), top=10)
#varImp(modRF3)
```

##Model Validation

Model #3 was used to predict the exercise techniques of the observations in the validation data set in order to validate the efectiveness of the model outside of the training set. A confusion matrix of the results is shown below.

```{r validate-model, include=TRUE}
#predRF1 <- predict(modRF1, vdata)
#cmRF1 <- confusionMatrix(predRF1, vdata$classe)
#predRF2 <- predict(modRF2, vdata)
#cmRF2 <- confusionMatrix(predRF2, vdata$classe)
predRF3 <- predict(modRF3, vdata)
cmRF3 <- confusionMatrix(predRF3, vdata$classe)
print(cmRF3)
```

The results proved model #3 to be equally accurate at classifying the validation set. The next step was to try the model on the real test data set.

##Testing Model Accuracy With Test Data Set
```{r load-testing-data}
# NOTE: source data file must be located in the working directory with the .Rmd file.
rawtest <- fread('./pml-testing.csv', na.strings=c('NA', '', '#DIV/0!'))
```
```{r preproc-testing-data}
# Pre-processing the testing data is done in the exact same way as the training data.

# Slice off the non-numeric data we don't care about.
ptstdata <- subset(rawtest, select=c(8:160))

# Convert all numbers stored as character strings into real numeric values.
ptstdata[,1:152] <- as.data.frame(sapply(subset(ptstdata, select=c(1:152)), as.numeric))

# Set all NA values to zero.
ptstdata[is.na(ptstdata)] <- 0
```
```{r predict-test-outcomes}
predFinalTest <- predict(modRF3, ptstdata)
```

The test data set consisted of 20 records selected from the WLE data set. This data set contained the predictor measurements only and did not include the true exercise technique identifying variable.

After the data was loaded and pre-processed in the same manner as the training/validation data, model #3 was used to classify the exercise technique for each observation. The model's clasification results were blind compared with the correct classifications (which were not known nor accessible ahead of time). The result was a perfect 20/20 match.

##Conclusion

The goal of this analysis was to create an accurate model for identifying correct and incorrect dumbbell bicep curl exercise techniques using the WLE data set. This was achieved with greater than 99% accuracy using a random forest method to create the model. Three separate models were compared. The best one was used to classify correct/incorrect exercise techniques in a blind test set with 100% accuracy.